{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "StandardScaler\n",
    "MinMaxScaler\n",
    "RobustScaler\n",
    "Normalizer\n",
    "\n",
    "\n",
    "## Standard Scaler\n",
    "The StandardScaler assumes your data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1.\n",
    "\n",
    "The mean and standard deviation are calculated for the feature and then the feature is scaled based on:\n",
    "\n",
    "xi–mean(x)/(stdev(x))\n",
    "\n",
    "If data is not normally distributed, this is not the best scaler to use.\n",
    "\n",
    "Let’s take a look at it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "np.random.seed(1)\n",
    "df = pd.DataFrame({\n",
    "    'x1': np.random.normal(0, 2, 10000),\n",
    "    'x2': np.random.normal(5, 3, 10000),\n",
    "    'x3': np.random.normal(-5, 5, 10000)\n",
    "})\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=['x1', 'x2', 'x3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaler\n",
    "The MinMaxScaler is the probably the most famous scaling algorithm, and follows the following formula for each feature:\n",
    "\n",
    "xi–min(x)/(max(x)–min(x))\n",
    "\n",
    "It essentially shrinks the range such that the range is now between 0 and 1 (or -1 to 1 if there are negative values).\n",
    "\n",
    "This scaler works better for cases in which the standard scaler might not work so well. If the distribution is not Gaussian or the standard deviation is very small, the min-max scaler works better.\n",
    "\n",
    "However, it is sensitive to outliers, so if there are outliers in the data, you might want to consider the Robust Scaler below.\n",
    "\n",
    "For now, let’s see the min-max scaler in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    # positive skew\n",
    "    'x1': np.random.chisquare(8, 1000),\n",
    "    # negative skew \n",
    "    'x2': np.random.beta(8, 2, 1000) * 40,\n",
    "    # no skew\n",
    "    'x3': np.random.normal(50, 3, 1000)\n",
    "})\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=['x1', 'x2', 'x3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Scaler\n",
    "The RobustScaler uses a similar method to the Min-Max scaler but it instead uses the interquartile range, rathar than the min-max, so that it is robust to outliers. Therefore it follows the formula:\n",
    "\n",
    "xi–Q1(x)/(Q3(x)–Q1(x))\n",
    "\n",
    "For each feature.\n",
    "\n",
    "Of course this means it is using the less of the data for scaling so it’s more suitable for when there are outliers in the data.\n",
    "\n",
    "Let’s take a look at this one in action on some data with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame({\n",
    "    # Distribution with lower outliers\n",
    "    'x1': np.concatenate([np.random.normal(20, 1, 1000), np.random.normal(1, 1, 25)]),\n",
    "    # Distribution with higher outliers\n",
    "    'x2': np.concatenate([np.random.normal(30, 1, 1000), np.random.normal(50, 1, 25)]),\n",
    "})\n",
    "\n",
    "scaler = preprocessing.RobustScaler()\n",
    "robust_scaled_df = scaler.fit_transform(x)\n",
    "robust_scaled_df = pd.DataFrame(robust_scaled_df, columns=['x1', 'x2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizer\n",
    "The normalizer scales each value by dividing each value by its magnitude in n-dimensional space for n number of features.\n",
    "\n",
    "Say your features were x, y and z Cartesian co-ordinates your scaled value for x would be:\n",
    "\n",
    "xi/ (sqrt(x^2(i)+y^2(i)+z^2(i))\n",
    "Each point is now within 1 unit of the origin on this Cartesian co-ordinate system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x1': np.random.randint(-100, 100, 1000).astype(float),\n",
    "    'y1': np.random.randint(-80, 80, 1000).astype(float),\n",
    "    'z1': np.random.randint(-150, 150, 1000).astype(float),\n",
    "})\n",
    "\n",
    "scaler = preprocessing.Normalizer()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
